#cloud-config

hostname: "MASTER_HOST_FQDN"
coreos:
  flannel:
    interface: MASTER_HOST_IP
  etcd2:
    discovery: https://discovery.etcd.io/DISCOVERY_ID
    advertise-client-urls: https://MASTER_HOST_IP:2379
    initial-advertise-peer-urls: https://MASTER_HOST_IP:2380
    listen-client-urls: http://127.0.0.1:2379,https://MASTER_HOST_IP:2379
    listen-peer-urls: https://MASTER_HOST_IP:2380
    cert-file: /etc/kubernetes/ssl/etcd-apiserver.pem
    key-file: /etc/kubernetes/ssl/etcd-apiserver-key.pem
    trusted-ca-file: /etc/kubernetes/ssl/etcd-ca.pem
    client-cert-auth: true
    peer-cert-file: /etc/kubernetes/ssl/etcd-apiserver.pem
    peer-key-file: /etc/kubernetes/ssl/etcd-apiserver-key.pem
    peer-trusted-ca-file: /etc/kubernetes/ssl/etcd-ca.pem
    peer-client-cert-auth: true
  fleet:
    metadata: "role=node"
  units:
    - name: etcd2.service
      command: stop
    - name: fleet.service
      command: stop
    - name: iptables-restore.service
      command: stop
    - name: 00-enp0s3.network
      runtime: true
      content: |
        [Match]
        Name=ens32
        [Network]
        DNS=DNSSERVER
        Address=MASTER_HOST_IP
        Gateway=MASTER_HOST_GW
    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
      command: start
    - name: flanneld.service
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            After=etcd2.service
            [Service]
            ExecStartPre=/usr/bin/etcdctl --cert-file=/etc/kubernetes/ssl/etcd-apiserver.pem --key-file=/etc/kubernetes/ssl/etcd-apiserver-key.pem --ca-file=/etc/kubernetes/ssl/etcd-ca.pem set /coreos.com/network/config '{ "Network": "10.2.0.0/16", "Backend":{"Type":"vxlan"}}'
            ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
      command: start
    - name: kubelet.service
      content: |
        [Unit]
        Requires=docker.service
        After=docker.service
        [Service]
        Environment=KUBELET_IMAGE_TAG=K8S_VER
        Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf"
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --api-servers=http://127.0.0.1:8080 \
        --register-schedulable=false \
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
        --network-plugin=cni \
        --container-runtime=docker \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --hostname-override=MASTER_HOST_IP \
        --cluster_dns=CLUSTER_DNS \
        --cluster_domain=cluster.local \
        --client-ca-file=/etc/kubernetes/ssl/ca.pem \
        --anonymous-auth=false \
        --cloud-provider=CLOUD_PROVIDER \
        --cloud-config=/etc/kubernetes/cloud.conf
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
      command: start
    - name: increase-nf_conntrack-connections.service
      command: start
      content: |
        [Unit]
        Description=Increase the number of connections in nf_conntrack.
        [Service]
        Type=oneshot
        ExecStartPre=/usr/sbin/modprobe nf_conntrack
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_max=589824"
    - name: increase-nf_conntrack-hashsize.service
      command: start
      content: |
        [Unit]
        Description=Increase the nf_conntrack hashsize.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 147456 > /sys/module/nf_conntrack/parameters/hashsize"
    - name: increase-port_range.service
      command: start
      content: |
        [Unit]
        Description=Increase port_range.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range"
    - name: increase-net.core.somaxconn.service
      command: start
      content: |
        [Unit]
        Description=Increase net.core.somaxconn.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.core.somaxconn=256"
    - name: change-conntrack_timeout.service
      command: start
      content: |
        [Unit]
        Description=change conntrack tcp timeout.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=1"
    - name: change-tcp_timeout_estab.service
      command: start
      content: |
        [Unit]
        Description=change tcp timeout estab.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=600"
    - name: settimezone.service
      command: start
      content: |
        [Unit]
        Description=Set the time zone
        [Service]
        ExecStart=/usr/bin/timedatectl set-timezone Europe/Amsterdam
        RemainAfterExit=yes
        Type=oneshot
    - name: systemd-timesyncd.service
      command: stop
      mask: true
    - name: ntpd.service
      command: start
      enable: true
    - name: systemd-modules-load.service
      command: restart
    - name: systemd-sysctl.service
      command: restart
  update:
    reboot-strategy: "etcd-lock"
  locksmith:
    window-start: Thu 04:00
    window-length: 1h
users:
  - name: "core"
    passwd: "USER_CORE_PASSWORD"
    groups:
      - "sudo"
      - "docker"
    ssh_authorized_keys:
      - USER_CORE_SSHKEY1
      - USER_CORE_SSHKEY2
write_files:
  - path: "/home/core/.bashrc"
    permissions: "0644"
    owner: "core"
    content: |
      if [[ $- != *i* ]] ; then
        return
      fi
      export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:$PWD
  - path: "/root/.bashrc"
    permissions: "0644"
    owner: "core"
    content: |
      if [[ $- != *i* ]] ; then
        return
      fi
      export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:$PWD:/home/core
  - path: "/var/lib/iptables/rules-save"
    permissions: "0644"
    owner: "root"
    content: |
      *filter
      :INPUT DROP [0:0]
      :FORWARD DROP [0:0]
      :OUTPUT ACCEPT [0:0]
      -A INPUT -i lo -j ACCEPT
      -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 443 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 2379 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 2380 -j ACCEPT
      -A INPUT -p udp -m udp --dport 8472 -j ACCEPT
      -A INPUT -p udp -m udp --dport 8285 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 3 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 11 -j ACCEPT
      COMMIT
  - path: "/etc/resolv.conf"
    permissions: "0644"
    owner: "root"
    content: |
      nameserver DNSSERVER
  - path: "/home/core/getkube.sh"
    permissions: "0644"
    owner: "core"
    content: |
     #!/bin/bash
     curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
     chmod +x kubectl
  - path: "/etc/kubernetes/cni/docker_opts_cni.env"
    permissions: "0644"
    owner: "root"
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""
  - path: "/etc/kubernetes/cni/net.d/10-flannel.conf"
    permissions: "0644"
    owner: "root"
    content: |
      {
        "name": "podnet",
        "type": "flannel",
        "delegate": {
          "isDefaultGateway": true
        }
      }
  - path: "/etc/flannel/options.env"
    permissions: "0644"
    owner: "root"
    content: |
      FLANNELD_IFACE=MASTER_HOST_IP
      FLANNELD_ETCD_ENDPOINTS=ETCD_ENDPOINTS_URLS
      FLANNELD_ETCD_KEYFILE=/etc/ssl/certs/etcd-apiserver-key.pem
      FLANNELD_ETCD_CERTFILE=/etc/ssl/certs/etcd-apiserver.pem
      FLANNELD_ETCD_CAFILE=/etc/ssl/certs/etcd-ca.pem
      FLANNEL_IMAGE_TAG=FLANNEL_VER
  - path: "/etc/kubernetes/manifests/kube-apiserver.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: quay.io/coreos/hyperkube:K8S_VER
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers=ETCD_ENDPOINTS_URLS
          - --etcd-cafile=/etc/kubernetes/ssl/etcd-ca.pem
          - --etcd-certfile=/etc/kubernetes/ssl/etcd-apiserver.pem
          - --etcd-keyfile=/etc/kubernetes/ssl/etcd-apiserver-key.pem
          - --allow-privileged=true
          - --storage-backend=etcd2
          - --service-cluster-ip-range=SERVICE_CLUSTER_IP_RANGE
          - --secure-port=443
          - --advertise-address=MASTER_HOST_IP
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1/networkpolicies=true
          - --kubelet-client-certificate=/etc/kubernetes/ssl/apiserver.pem
          - --kubelet-client-key=/etc/kubernetes/ssl/apiserver-key.pem
          - --anonymous-auth=false
          - --cloud-provider=CLOUD_PROVIDER
          - --cloud-config=/etc/kubernetes/ssl/cloud.conf
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: "/etc/kubernetes/manifests/kube-proxy.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:K8S_VER
          command:
          - /hyperkube
          - proxy
          - --master=http://127.0.0.1:8080
          - --cluster-cidr=10.2.0.0/16
          - --conntrack-max-per-core=0
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: "/etc/kubernetes/manifests/kube-controller-manager.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-controller-manager
          image: quay.io/coreos/hyperkube:K8S_VER
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --cloud-provider=CLOUD_PROVIDER
          - --cloud-config=/etc/kubernetes/ssl/cloud.conf
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
  - path: "/etc/kubernetes/manifests/kube-scheduler.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: quay.io/coreos/hyperkube:K8S_VER
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15
  - path: "/etc/kubernetes/manifests/calico.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      # This ConfigMap is used to configure a self-hosted Calico installation.
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: calico-config
        namespace: kube-system
      data:
        # Configure this with the location of your etcd cluster.
        etcd_endpoints: "ETCD_ENDPOINTS_URLS"

        # Configure the Calico backend to use.
        calico_backend: "bird"

        # The CNI network configuration to install on each node.
        cni_network_config: |-
          {
              "name": "k8s-pod-network",
              "type": "calico",
              "etcd_endpoints": "__ETCD_ENDPOINTS__",
              "etcd_key_file": "__ETCD_KEY_FILE__",
              "etcd_cert_file": "__ETCD_CERT_FILE__",
              "etcd_ca_cert_file": "__ETCD_CA_CERT_FILE__",
              "log_level": "info",
              "ipam": {
                  "type": "calico-ipam"
              },
              "policy": {
                  "type": "k8s",
                  "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                  "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
              },
              "kubernetes": {
                  "kubeconfig": "__KUBECONFIG_FILEPATH__"
              }
          }

        # The default IP Pool to be created for the cluster.
        # Pod IP addresses will be assigned from this pool.
        ippool.yaml: |
            apiVersion: v1
            kind: ipPool
            metadata:
              cidr: 192.168.0.0/16
            spec:
              nat-outgoing: true

        # If you're using TLS enabled etcd uncomment the following.
        # You must also populate the Secret below with these files.
        etcd_ca: "/calico-secrets/etcd-ca"   # "/calico-secrets/etcd-ca"
        etcd_cert: "/calico-secrets/etcd-cert" # "/calico-secrets/etcd-cert"
        etcd_key: "/calico-secrets/etcd-key"  # "/calico-secrets/etcd-key"

      ---

      # The following contains k8s Secrets for use with a TLS enabled etcd cluster.
      # For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/
      apiVersion: v1
      kind: Secret
      type: Opaque
      metadata:
        name: calico-etcd-secrets
        namespace: kube-system
      data:
        # Populate the following files with etcd TLS configuration if desired, but leave blank if
        # not using TLS for etcd.
        # This self-hosted install expects three files with the following names.  The values
        # should be base64 encoded strings of the entire contents of each file.
        etcd-key: ETCDAPISERVERKEYBASE64
        etcd-cert: ETCDAPISERVERBASE64
        etcd-ca: ETCDCACERTBASE64

      ---

      # This manifest installs the calico/node container, as well
      # as the Calico CNI plugins and network config on
      # each master and worker node in a Kubernetes cluster.
      kind: DaemonSet
      apiVersion: extensions/v1beta1
      metadata:
        name: calico-node
        namespace: kube-system
        labels:
          k8s-app: calico-node
      spec:
        selector:
          matchLabels:
            k8s-app: calico-node
        template:
          metadata:
            labels:
              k8s-app: calico-node
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            hostNetwork: true
            containers:
              # Runs calico/node container on each Kubernetes node.  This
              # container programs network policy and routes on each
              # host.
              - name: calico-node
                image: quay.io/calico/node:v1.0.2
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # Choose the backend to use.
                  - name: CALICO_NETWORKING_BACKEND
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: calico_backend
                  # Disable file logging so `kubectl logs` works.
                  - name: CALICO_DISABLE_FILE_LOGGING
                    value: "true"
                  # Set Felix endpoint to host default action to ACCEPT.
                  - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                    value: "ACCEPT"
                  # Don't configure a default pool.  This is done by the Job
                  # below.
                  - name: NO_DEFAULT_POOLS
                    value: "true"
                  - name: FELIX_LOGSEVERITYSCREEN
                    value: "info"
                  # Location of the CA certificate for etcd.
                  - name: ETCD_CA_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_ca
                  # Location of the client key for etcd.
                  - name: ETCD_KEY_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_key
                  # Location of the client certificate for etcd.
                  - name: ETCD_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_cert
                  # Auto-detect the BGP IP address.
                  - name: IP
                    value: ""
                securityContext:
                  privileged: true
                volumeMounts:
                  - mountPath: /lib/modules
                    name: lib-modules
                    readOnly: true
                  - mountPath: /var/run/calico
                    name: var-run-calico
                    readOnly: false
                  - mountPath: /calico-secrets
                    name: etcd-certs
              # This container installs the Calico CNI binaries
              # and CNI network config file on each node.
              - name: install-cni
                image: calico/cni:v1.5.6
                command: ["/install-cni.sh"]
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # The CNI network config to install on each node.
                  - name: CNI_NETWORK_CONFIG
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: cni_network_config
                volumeMounts:
                  - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                  - mountPath: /host/etc/cni/net.d
                    name: cni-net-dir
                  - mountPath: /calico-secrets
                    name: etcd-certs
            volumes:
              # Used by calico/node.
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-run-calico
                hostPath:
                  path: /var/run/calico
              # Used to install CNI.
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: cni-net-dir
                hostPath:
                  path: /etc/cni/net.d
              # Mount in the etcd TLS secrets.
              - name: etcd-certs
                secret:
                  secretName: calico-etcd-secrets

      ---

      # This manifest deploys the Calico policy controller on Kubernetes.
      # See https://github.com/projectcalico/k8s-policy
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: calico-policy-controller
        namespace: kube-system
        labels:
          k8s-app: calico-policy
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ''
          scheduler.alpha.kubernetes.io/tolerations: |
            [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
             {"key":"CriticalAddonsOnly", "operator":"Exists"}]
      spec:
        # The policy controller can only have a single active instance.
        replicas: 1
        strategy:
          type: Recreate
        template:
          metadata:
            name: calico-policy-controller
            namespace: kube-system
            labels:
              k8s-app: calico-policy
          spec:
            # The policy controller must run in the host network namespace so that
            # it isn't governed by policy that would prevent it from working.
            hostNetwork: true
            containers:
              - name: calico-policy-controller
                image: calico/kube-policy-controller:v0.5.2
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # Location of the CA certificate for etcd.
                  - name: ETCD_CA_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_ca
                  # Location of the client key for etcd.
                  - name: ETCD_KEY_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_key
                  # Location of the client certificate for etcd.
                  - name: ETCD_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_cert
                  # The location of the Kubernetes API.  Use the default Kubernetes
                  # service for API access.
                  - name: K8S_API
                    value: "https://kubernetes.default:443"
                  # Since we're running in the host namespace and might not have KubeDNS
                  # access, configure the container's /etc/hosts to resolve
                  # kubernetes.default to the correct service clusterIP.
                  - name: CONFIGURE_ETC_HOSTS
                    value: "true"
                volumeMounts:
                  # Mount in the etcd TLS secrets.
                  - mountPath: /calico-secrets
                    name: etcd-certs
            volumes:
              # Mount in the etcd TLS secrets.
              - name: etcd-certs
                secret:
                  secretName: calico-etcd-secrets

      ---

      ## This manifest deploys a Job which performs one time
      # configuration of Calico
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: configure-calico
        namespace: kube-system
        labels:
          k8s-app: calico
      spec:
        template:
          metadata:
            name: configure-calico
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            hostNetwork: true
            restartPolicy: OnFailure
            containers:
              # Writes basic configuration to datastore.
              - name: configure-calico
                image: calico/ctl:v1.0.2
                args:
                - apply
                - -f
                - /etc/config/calico/ippool.yaml
                volumeMounts:
                  - name: config-volume
                    mountPath: /etc/config
                  # Mount in the etcd TLS secrets.
                  - mountPath: /calico-secrets
                    name: etcd-certs
                env:
                  # The location of the etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # Location of the CA certificate for etcd.
                  - name: ETCD_CA_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_ca
                  # Location of the client key for etcd.
                  - name: ETCD_KEY_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_key
                  # Location of the client certificate for etcd.
                  - name: ETCD_CERT_FILE
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_cert
            volumes:
              - name: config-volume
                configMap:
                  name: calico-config
                  items:
                  - key: ippool.yaml
                    path: calico/ippool.yaml
              # Mount in the etcd TLS secrets.
              - name: etcd-certs
                secret:
                  secretName: calico-etcd-secrets
  - path: "/etc/kubernetes/ssl/apiserver-key.pem"
    permissions: "0600"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      APISERVERKEY
  - path: "/etc/kubernetes/ssl/apiserver.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      APISERVER
  - path: "/etc/kubernetes/ssl/ca.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      CACERT
  - path: "/etc/kubernetes/ssl/etcd-apiserver-key.pem"
    permissions: "0644"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDAPISERVERKEY
  - path: "/etc/kubernetes/ssl/etcd-apiserver.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDAPISERVER
  - path: "/etc/kubernetes/ssl/etcd-ca.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDCACERT
  - path: "/etc/ssl/certs/etcd-apiserver-key.pem"
    permissions: "0644"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDAPISERVERKEY
  - path: "/etc/ssl/certs/etcd-apiserver.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDAPISERVER
  - path: "/etc/ssl/certs/etcd-ca.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDCACERT
  - path: "/etc/kubernetes/cloud.conf"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      CLOUDCONF
  - path: "/etc/kubernetes/ssl/cloud.conf"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      CLOUDCONF
  - path: /etc/motd.d/k8s.conf
    owner: "root"
    permissions: "0644"
    encoding: "gzip+base64"
    content: |
      H4sIAJ7IHVkAA3WTO7rEIAiF+6zCng/oWQ07oKDK7u9Bx6iTuTTx8R8CAq19mzOLv05h1/eBsEbed2YYvxQ77WKUIKdBoafi+kkSP8tSyEmfPu/b2rGFwhfN5xWRNKU4BLHo6HuxvJPY3aUiw0ejDqzjPGn++DRBTkAyERt+YRAIUy7n13RdvDTQuC261BUszVhk0DL33CRUqjaIxGoR2nz6okFPdfZlasVWz8zRPdrm/Go+c0dkpFaP61EalECJ16+1aO1+FVzVXZitQCwhrG5xg6gzDjo/dCDIHhUViXSrWyqVoEEjr6utIt/qbhFeR+R4RFRJZpJ4+fLdXB8+qjYgqp/wBzz+djVruRygov5YW2yORhld5TbPzQl1gWUmP15Czo6VkUjyUzts7HNo72ngo+c2Da2B2GZny3Zje3a/Jg0z+TLZ799T/K9df0AUN+scBAAA
  - path: /etc/ntp.conf
    content: |
      server 0.europe.pool.ntp.org
      server 1.europe.pool.ntp.org
      server 2.europe.pool.ntp.org
      # - Allow only time queries, at a limited rate.
      # - Allow all local queries (IPv4, IPv6)
      restrict default nomodify nopeer noquery limited kod
      restrict 127.0.0.1
      restrict [::1]
