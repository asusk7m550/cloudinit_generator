#cloud-config

hostname: "WORKER_IP"
coreos:
  flannel:
    interface: WORKER_IP
  etcd2:
    discovery: https://discovery.etcd.io/DISCOVERY_ID
    advertise-client-urls: https://WORKER_IP:2379
    initial-advertise-peer-urls: https://WORKER_IP:2380
    listen-client-urls: http://127.0.0.1:2379,https://WORKER_IP:2379
    listen-peer-urls: https://WORKER_IP:2380
    cert-file: /etc/kubernetes/ssl/etcd-worker.pem
    key-file: /etc/kubernetes/ssl/etcd-worker-key.pem
    trusted-ca-file: /etc/kubernetes/ssl/etcd-ca.pem
    client-cert-auth: true
    peer-cert-file: /etc/kubernetes/ssl/etcd-worker.pem
    peer-key-file: /etc/kubernetes/ssl/etcd-worker-key.pem
    peer-trusted-ca-file: /etc/kubernetes/ssl/etcd-ca.pem
    peer-client-cert-auth: true
    proxy: on
  fleet:
    metadata: "role=node"
  units:
    - name: etcd2.service
      command: stop
    - name: fleet.service
      command: stop
    - name: iptables-restore.service
      command: stop
    - name: 00-enp0s3.network
      runtime: true
      content: |
        [Match]
        Name=ens32
        [Network]
        DNS=DNSSERVER
        Address=WORKER_IP
        Gateway=WORKER_GW
    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
      command: start
    - name: flanneld.service
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            After=etcd2.service
            [Service]
            ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
      command: start
    - name: kubelet.service
      content: |
        [Unit]
        Requires=docker.service
        After=docker.service
        [Service]
        Environment=KUBELET_IMAGE_TAG=K8S_VER
        Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log"
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --api-servers=https://MASTER_HOST_IP \
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
        --container-runtime=docker \
        --network-plugin=cni \
        --register-node=true \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --hostname-override=WORKER_IP \
        --cluster_dns=CLUSTER_DNS \
        --cluster_domain=cluster.local \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
        --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
        --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem \
        --client-ca-file=/etc/kubernetes/ssl/ca.pem \
        --anonymous-auth=false \
        --cloud-provider=CLOUD_PROVIDER \
        --cloud-config=/etc/kubernetes/ssl/cloud.conf
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
      command: start
    - name: increase-nf_conntrack-connections.service
      command: start
      content: |
        [Unit]
        Description=Increase the number of connections in nf_conntrack.
        [Service]
        Type=oneshot
        ExecStartPre=/usr/sbin/modprobe nf_conntrack
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_max=589824"
    - name: increase-nf_conntrack-hashsize.service
      command: start
      content: |
        [Unit]
        Description=Increase the nf_conntrack hashsize.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 147456 > /sys/module/nf_conntrack/parameters/hashsize"
    - name: increase-port_range.service
      command: start
      content: |
        [Unit]
        Description=Increase port_range.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 1024 65535 > /proc/sys/net/ipv4/ip_local_port_range"
    - name: increase-net.core.somaxconn.service
      command: start
      content: |
        [Unit]
        Description=Increase net.core.somaxconn.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.core.somaxconn=256"
    - name: change-conntrack_timeout.service
      command: start
      content: |
        [Unit]
        Description=change conntrack tcp timeout.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=1"
    - name: change-tcp_timeout_estab.service
      command: start
      content: |
        [Unit]
        Description=change tcp timeout estab.
        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=600"
    - name: settimezone.service
      command: start
      content: |
        [Unit]
        Description=Set the time zone
        [Service]
        ExecStart=/usr/bin/timedatectl set-timezone Europe/Amsterdam
        RemainAfterExit=yes
        Type=oneshot
    - name: systemd-timesyncd.service
      command: stop
      mask: true
    - name: ntpd.service
      command: start
      enable: true
    - name: systemd-modules-load.service
      command: restart
    - name: systemd-sysctl.service
      command: restart
  update:
    reboot-strategy: "etcd-lock"
  locksmith:
    window-start: Thu 04:00
    window-length: 1h
users:
  - name: "core"
    passwd: "USER_CORE_PASSWORD"
    groups:
      - "sudo"
      - "docker"
    ssh_authorized_keys:
      - USER_CORE_SSHKEY1
      - USER_CORE_SSHKEY2
write_files:
  - path: "/home/core/.bashrc"
    permissions: "0644"
    owner: "core"
    content: |
      if [[ $- != *i* ]] ; then
        return
      fi
      export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:$PWD
  - path: "/root/.bashrc"
    permissions: "0644"
    owner: "core"
    content: |
      if [[ $- != *i* ]] ; then
        return
      fi
      export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/bin:$PWD:/home/core
  - path: "/var/lib/iptables/rules-save"
    permissions: "0644"
    owner: "root"
    content: |
      *filter
      :INPUT DROP [0:0]
      :FORWARD DROP [0:0]
      :OUTPUT ACCEPT [0:0]
      -A INPUT -i lo -j ACCEPT
      -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 443 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 2379 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 2380 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 10250 -j ACCEPT
      -A INPUT -p tcp -m tcp --dport 10255 -j ACCEPT
      -A INPUT -p tcp --match multiport --dports 30000:32767 -j ACCEPT
      -A INPUT -p udp -m udp --dport 8285 -j ACCEPT
      -A INPUT -p udp -m udp --dport 8472 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 0 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 3 -j ACCEPT
      -A INPUT -p icmp -m icmp --icmp-type 11 -j ACCEPT
      COMMIT
  - path: "/etc/resolv.conf"
    permissions: "0644"
    owner: "root"
    content: |
      nameserver DNSSERVER
  - path: "/etc/kubernetes/cni/docker_opts_cni.env"
    permissions: "0644"
    owner: "root"
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""
  - path: "/etc/kubernetes/cni/net.d/10-flannel.conf"
    permissions: "0644"
    owner: "root"
    content: |
      {
        "name": "podnet",
        "type": "flannel",
        "delegate": {
          "isDefaultGateway": true
        }
      }
  - path: "/etc/flannel/options.env"
    permissions: "0644"
    owner: "root"
    content: |
      FLANNELD_IFACE=WORKER_IP
      FLANNELD_ETCD_ENDPOINTS=ETCD_ENDPOINTS_URLS
      FLANNELD_ETCD_KEYFILE=/etc/ssl/certs/etcd-worker-key.pem
      FLANNELD_ETCD_CERTFILE=/etc/ssl/certs/etcd-worker.pem
      FLANNELD_ETCD_CAFILE=/etc/ssl/certs/etcd-ca.pem
      FLANNEL_IMAGE_TAG=FLANNEL_VER
  - path: "/etc/kubernetes/manifests/kube-proxy.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: quay.io/coreos/hyperkube:K8S_VER
          command:
          - /hyperkube
          - proxy
          - --master=https://MASTER_HOST_IP
          - --cluster-cidr=10.2.0.0/16
          - --conntrack-max-per-core=0
          - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
          securityContext:
            privileged: true
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: "ssl-certs"
          - mountPath: /etc/kubernetes/worker-kubeconfig.yaml
            name: "kubeconfig"
            readOnly: true
          - mountPath: /etc/kubernetes/ssl
            name: "etc-kube-ssl"
            readOnly: true
        volumes:
        - name: "ssl-certs"
          hostPath:
            path: "/usr/share/ca-certificates"
        - name: "kubeconfig"
          hostPath:
            path: "/etc/kubernetes/worker-kubeconfig.yaml"
        - name: "etc-kube-ssl"
          hostPath:
            path: "/etc/kubernetes/ssl"
  - path: "/etc/kubernetes/worker-kubeconfig.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/worker.pem
          client-key: /etc/kubernetes/ssl/worker-key.pem
      contexts:
      - context:
          cluster: local
          user: kubelet
        name: kubelet-context
      current-context: kubelet-context
  - path: "/etc/kubernetes/manifests/calico.yaml"
    permissions: "0644"
    owner: "root"
    content: |
      # This ConfigMap is used to configure a self-hosted Calico installation.
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: calico-config
        namespace: kube-system
      data:
        # Configure this with the location of your etcd cluster.
        etcd_endpoints: "ETCD_ENDPOINTS_URLS"

        # The CNI network configuration to install on each node.  The special
        # values in this config will be automatically populated.
        cni_network_config: |-
          {
              "name": "calico",
              "type": "flannel",
              "delegate": {
                "type": "calico",
                "etcd_endpoints": "__ETCD_ENDPOINTS__",
                "log_level": "info",
                "policy": {
                    "type": "k8s",
                    "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                    "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
                },
                "kubernetes": {
                    "kubeconfig": "/etc/kubernetes/cni/net.d/__KUBECONFIG_FILENAME__"
                }
              }
          }

      ---

      # This manifest installs the calico/node container, as well
      # as the Calico CNI plugins and network config on
      # each master and worker node in a Kubernetes cluster.
      kind: DaemonSet
      apiVersion: extensions/v1beta1
      metadata:
        name: calico-node
        namespace: kube-system
        labels:
          k8s-app: calico-node
      spec:
        selector:
          matchLabels:
            k8s-app: calico-node
        template:
          metadata:
            labels:
              k8s-app: calico-node
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            hostNetwork: true
            containers:
              # Runs calico/node container on each Kubernetes node.  This
              # container programs network policy and routes on each
              # host.
              - name: calico-node
                image: quay.io/calico/node:latest
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # Choose the backend to use.
                  - name: CALICO_NETWORKING_BACKEND
                    value: "none"
                  # Disable file logging so `kubectl logs` works.
                  - name: CALICO_DISABLE_FILE_LOGGING
                    value: "true"
                  - name: NO_DEFAULT_POOLS
                    value: "true"
                securityContext:
                  privileged: true
                volumeMounts:
                  - mountPath: /lib/modules
                    name: lib-modules
                    readOnly: false
                  - mountPath: /var/run/calico
                    name: var-run-calico
                    readOnly: false
                  - mountPath: /etc/resolv.conf
                    name: dns
                    readOnly: true
              # This container installs the Calico CNI binaries
              # and CNI network config file on each node.
              - name: install-cni
                image: quay.io/calico/cni:latest
                imagePullPolicy: Always
                command: ["/install-cni.sh"]
                env:
                  # CNI configuration filename
                  - name: CNI_CONF_NAME
                    value: "10-calico.conf"
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # The CNI network config to install on each node.
                  - name: CNI_NETWORK_CONFIG
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: cni_network_config
                volumeMounts:
                  - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                  - mountPath: /host/etc/cni/net.d
                    name: cni-net-dir
            volumes:
              # Used by calico/node.
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-run-calico
                hostPath:
                  path: /var/run/calico
              # Used to install CNI.
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: cni-net-dir
                hostPath:
                  path: /etc/kubernetes/cni/net.d
              - name: dns
                hostPath:
                  path: /etc/resolv.conf

      ---

      # This manifest deploys the Calico policy controller on Kubernetes.
      # See https://github.com/projectcalico/k8s-policy
      apiVersion: extensions/v1beta1
      kind: ReplicaSet
      metadata:
        name: calico-policy-controller
        namespace: kube-system
        labels:
          k8s-app: calico-policy
      spec:
        # The policy controller can only have a single active instance.
        replicas: 1
        template:
          metadata:
            name: calico-policy-controller
            namespace: kube-system
            labels:
              k8s-app: calico-policy
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            # The policy controller must run in the host network namespace so that
            # it isn't governed by policy that would prevent it from working.
            hostNetwork: true
            containers:
              - name: calico-policy-controller
                image: calico/kube-policy-controller:master
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # The location of the Kubernetes API.  Use the default Kubernetes
                  # service for API access.
                  - name: K8S_API
                    value: "https://kubernetes.default:443"
                  # Since we're running in the host namespace and might not have KubeDNS
                  # access, configure the container's /etc/hosts to resolve
                  # kubernetes.default to the correct service clusterIP.
                  - name: CONFIGURE_ETC_HOSTS
                    value: "true"      
  - path: "/etc/kubernetes/ssl/ca.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      CACERT
  - path: "/etc/kubernetes/ssl/worker-key.pem"
    permissions: "0600"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      WORKERKEY
  - path: "/etc/kubernetes/ssl/worker.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      WORKER
  - path: "/etc/kubernetes/ssl/etcd-ca.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDCACERT
  - path: "/etc/kubernetes/ssl/etcd-worker-key.pem"
    permissions: "0644"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDWORKERKEY
  - path: "/etc/kubernetes/ssl/etcd-worker.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDWORKER
  - path: "/etc/ssl/certs/etcd-ca.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDCACERT
  - path: "/etc/ssl/certs/etcd-worker-key.pem"
    permissions: "0644"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDWORKERKEY
  - path: "/etc/ssl/certs/etcd-worker.pem"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      ETCDWORKER
  - path: "/etc/kubernetes/ssl/cloud.conf"
    permissions: "0664"
    encoding: "gzip+base64"
    owner: "root"
    content: |
      CLOUDCONF
  - path: /etc/motd.d/k8s.conf
    owner: "root"
    permissions: "0644"
    encoding: "gzip+base64"
    content: |
      H4sIAJ7IHVkAA3WTO7rEIAiF+6zCng/oWQ07oKDK7u9Bx6iTuTTx8R8CAq19mzOLv05h1/eBsEbed2YYvxQ77WKUIKdBoafi+kkSP8tSyEmfPu/b2rGFwhfN5xWRNKU4BLHo6HuxvJPY3aUiw0ejDqzjPGn++DRBTkAyERt+YRAIUy7n13RdvDTQuC261BUszVhk0DL33CRUqjaIxGoR2nz6okFPdfZlasVWz8zRPdrm/Go+c0dkpFaP61EalECJ16+1aO1+FVzVXZitQCwhrG5xg6gzDjo/dCDIHhUViXSrWyqVoEEjr6utIt/qbhFeR+R4RFRJZpJ4+fLdXB8+qjYgqp/wBzz+djVruRygov5YW2yORhld5TbPzQl1gWUmP15Czo6VkUjyUzts7HNo72ngo+c2Da2B2GZny3Zje3a/Jg0z+TLZ799T/K9df0AUN+scBAAA
  - path: /etc/ntp.conf
    content: |
      server 0.europe.pool.ntp.org
      server 1.europe.pool.ntp.org
      server 2.europe.pool.ntp.org
      # - Allow only time queries, at a limited rate.
      # - Allow all local queries (IPv4, IPv6)
      restrict default nomodify nopeer noquery limited kod
      restrict 127.0.0.1
      restrict [::1]
